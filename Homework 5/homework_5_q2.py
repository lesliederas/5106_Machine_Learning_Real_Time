# -*- coding: utf-8 -*-
"""Homework_5_Q2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AEJ4bojWZE9xLaTiZzWmb4-0BHr6iVMb

Part 1
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import requests
from torch.utils.data import Dataset, DataLoader

# Step 1: Download the dataset
url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
response = requests.get(url)
text = response.text  # This is the entire text data

# Step 2: Prepare the dataset
sequence_length = 20
# Create a character mapping to integers
chars = sorted(list(set(text)))
char_to_int = {ch: i for i, ch in enumerate(chars)}
int_to_char = {i: ch for i, ch in enumerate(chars)}

# Encode the text into integers
encoded_text = [char_to_int[ch] for ch in text]

# Create sequences and targets
def create_sequences(sequence_length, encoded_text):
    sequences = []
    targets = []
    for i in range(0, len(encoded_text) - sequence_length):
        seq = encoded_text[i:i+sequence_length]
        target = encoded_text[i+sequence_length]
        sequences.append(seq)
        targets.append(target)
    return torch.tensor(sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long)

sequences, targets = create_sequences(sequence_length, encoded_text)

# Step 3: Create a dataset class
class CharDataset(Dataset):
    def __init__(self, sequences, targets):
        self.sequences = sequences
        self.targets = targets

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, index):
        return self.sequences[index], self.targets[index]

# Instantiate the dataset
dataset = CharDataset(sequences, targets)

# Step 4: Create data loaders
batch_size = 128
train_size = int(len(dataset) * 0.8)
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)

# Transformer Model Definition
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, sequence_length):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length, d_model))
        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        x = self.embedding(x) + self.positional_encoding
        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, input_size)
        output = self.transformer(x, x)  # For language models, both inputs are the same
        output = self.fc_out(output[-1])  # Only predict the next token
        return output

# RNN Model Definition (LSTM)
class RNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):
        super(RNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        output, (hn, cn) = self.rnn(x)
        output = self.fc_out(output[:, -1, :])  # Only predict the last token
        return output

# Training and Evaluation functions
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for sequences, targets in train_loader:
        sequences, targets = sequences.to(device), targets.to(device)

        optimizer.zero_grad()
        output = model(sequences)

        loss = criterion(output, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(train_loader)

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for sequences, targets in test_loader:
            sequences, targets = sequences.to(device), targets.to(device)

            output = model(sequences)
            loss = criterion(output, targets)

            total_loss += loss.item()

            # Accuracy calculation
            _, predicted = torch.max(output, dim=1)
            correct += (predicted == targets).sum().item()
            total += targets.size(0)

    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total
    return avg_loss, accuracy

# Hyperparameters
sequence_lengths = [20, 30]
batch_size = 128
learning_rate = 0.001
d_model = 256  # Embedding dimension and transformer model dimension
nhead = 2  # Number of attention heads
num_layers = 2  # Number of transformer layers
vocab_size = len(char_to_int)

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Training loop for both models
for seq_len in sequence_lengths:
    print(f"Training for sequence length: {seq_len}")

    # Create sequences and targets for current sequence length
    sequences, targets = create_sequences(seq_len, encoded_text)
    dataset = CharDataset(sequences, targets)
    train_size = int(len(dataset) * 0.8)
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)

    # Transformer Model
    transformer_model = TransformerModel(vocab_size, d_model, nhead, num_layers, seq_len).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(transformer_model.parameters(), lr=learning_rate)

    # Train and evaluate Transformer model
    start_time = time.time()
    train_loss = train_model(transformer_model, train_loader, criterion, optimizer, device)
    end_time = time.time()

    test_loss, accuracy = evaluate_model(transformer_model, test_loader, criterion, device)
    execution_time = end_time - start_time

    print(f"Transformer | Seq Len: {seq_len} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Accuracy: {accuracy:.4f}")
    print(f"Training Time: {execution_time:.4f} seconds")

    # RNN Model (LSTM)
    rnn_model = RNNModel(vocab_size, d_model, 512, num_layers).to(device)
    optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)

    # Train and evaluate RNN model
    start_time = time.time()
    train_loss = train_model(rnn_model, train_loader, criterion, optimizer, device)
    end_time = time.time()

    test_loss, accuracy = evaluate_model(rnn_model, test_loader, criterion, device)
    execution_time = end_time - start_time

    print(f"RNN (LSTM) | Seq Len: {seq_len} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Accuracy: {accuracy:.4f}")
    print(f"Training Time: {execution_time:.4f} seconds")

"""Part 2"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import requests
from torch.utils.data import Dataset, DataLoader
import numpy as np

# Step 1: Download the dataset
url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
response = requests.get(url)
text = response.text  # This is the entire text data

# Step 2: Prepare the dataset
def create_sequences(sequence_length, encoded_text):
    sequences = []
    targets = []
    for i in range(0, len(encoded_text) - sequence_length):
        seq = encoded_text[i:i+sequence_length]
        target = encoded_text[i+sequence_length]
        sequences.append(seq)
        targets.append(target)
    return torch.tensor(sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long)

chars = sorted(list(set(text)))
char_to_int = {ch: i for i, ch in enumerate(chars)}
int_to_char = {i: ch for i, ch in enumerate(chars)}

encoded_text = [char_to_int[ch] for ch in text]

# Hyperparameters for sequence lengths and batch size
sequence_lengths = [20, 30]
batch_size = 128
learning_rate = 0.001
vocab_size = len(char_to_int)

# Transformer Model Definition
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, sequence_length):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length, d_model))
        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        x = self.embedding(x) + self.positional_encoding
        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, input_size)
        output = self.transformer(x, x)  # For language models, both inputs are the same
        output = self.fc_out(output[-1])  # Only predict the next token
        return output

# Data preparation for each sequence length
def prepare_data(sequence_length):
    sequences, targets = create_sequences(sequence_length, encoded_text)
    dataset = CharDataset(sequences, targets)
    train_size = int(len(dataset) * 0.8)
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)
    return train_loader, test_loader

# Training and Evaluation functions
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for sequences, targets in train_loader:
        sequences, targets = sequences.to(device), targets.to(device)

        optimizer.zero_grad()
        output = model(sequences)

        loss = criterion(output, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(train_loader)

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for sequences, targets in test_loader:
            sequences, targets = sequences.to(device), targets.to(device)

            output = model(sequences)
            loss = criterion(output, targets)

            total_loss += loss.item()

            # Accuracy calculation
            _, predicted = torch.max(output, dim=1)
            correct += (predicted == targets).sum().item()
            total += targets.size(0)

    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total
    return avg_loss, accuracy

# Experiment with different Transformer configurations
configurations = [(1, 2), (2, 2), (4, 2), (1, 4), (2, 4), (4, 4)]
sequence_lengths = [20, 30]

# Start experiment
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
results = []

for num_layers, nhead in configurations:
    for seq_len in sequence_lengths:
        print(f"Training Transformer with {num_layers} layers and {nhead} heads, Sequence Length: {seq_len}")

        # Prepare data
        train_loader, test_loader = prepare_data(seq_len)

        # Model initialization
        model = TransformerModel(vocab_size, 256, nhead, num_layers, seq_len).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        # Training the model
        start_time = time.time()
        train_loss = train_model(model, train_loader, criterion, optimizer, device)
        end_time = time.time()

        # Evaluation
        test_loss, accuracy = evaluate_model(model, test_loader, criterion, device)
        execution_time = end_time - start_time

        # Collect results
        model_size = sum(p.numel() for p in model.parameters())
        results.append({
            'layers': num_layers,
            'heads': nhead,
            'sequence_length': seq_len,
            'train_loss': train_loss,
            'test_loss': test_loss,
            'accuracy': accuracy,
            'execution_time': execution_time,
            'model_size': model_size
        })

        print(f"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Accuracy: {accuracy:.4f}")
        print(f"Execution Time: {execution_time:.4f} seconds | Model Size: {model_size} parameters")

# Print Summary of Results
print("\nSummary of Results:")
for result in results:
    print(f"Layers: {result['layers']} | Heads: {result['heads']} | Sequence Length: {result['sequence_length']}")
    print(f"Train Loss: {result['train_loss']:.4f} | Test Loss: {result['test_loss']:.4f} | Accuracy: {result['accuracy']:.4f}")
    print(f"Execution Time: {result['execution_time']:.4f} seconds | Model Size: {result['model_size']} parameters")
    print("-" * 50)

"""Sequence_Length = 50"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import requests
from torch.utils.data import Dataset, DataLoader
import numpy as np

# Step 1: Download the dataset
url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
response = requests.get(url)
text = response.text  # This is the entire text data

# Step 2: Prepare the dataset
def create_sequences(sequence_length, encoded_text):
    sequences = []
    targets = []
    for i in range(0, len(encoded_text) - sequence_length):
        seq = encoded_text[i:i+sequence_length]
        target = encoded_text[i+sequence_length]
        sequences.append(seq)
        targets.append(target)
    return torch.tensor(sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long)

chars = sorted(list(set(text)))
char_to_int = {ch: i for i, ch in enumerate(chars)}
int_to_char = {i: ch for i, ch in enumerate(chars)}

encoded_text = [char_to_int[ch] for ch in text]

# Hyperparameters for sequence lengths and batch size
sequence_lengths = [50]
batch_size = 128
learning_rate = 0.001
vocab_size = len(char_to_int)

# Transformer Model Definition
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, sequence_length):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length, d_model))
        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        x = self.embedding(x) + self.positional_encoding
        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, input_size)
        output = self.transformer(x, x)  # For language models, both inputs are the same
        output = self.fc_out(output[-1])  # Only predict the next token
        return output

# Data preparation for each sequence length
def prepare_data(sequence_length):
    sequences, targets = create_sequences(sequence_length, encoded_text)
    dataset = CharDataset(sequences, targets)
    train_size = int(len(dataset) * 0.8)
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)
    return train_loader, test_loader

# Training and Evaluation functions
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for sequences, targets in train_loader:
        sequences, targets = sequences.to(device), targets.to(device)

        optimizer.zero_grad()
        output = model(sequences)

        loss = criterion(output, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(train_loader)

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for sequences, targets in test_loader:
            sequences, targets = sequences.to(device), targets.to(device)

            output = model(sequences)
            loss = criterion(output, targets)

            total_loss += loss.item()

            # Accuracy calculation
            _, predicted = torch.max(output, dim=1)
            correct += (predicted == targets).sum().item()
            total += targets.size(0)

    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total
    return avg_loss, accuracy

# Experiment with different Transformer configurations
configurations = [(1, 2), (2, 2), (4, 2), (1, 4), (2, 4), (4, 4)]
sequence_lengths = [50]

# Start experiment
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
results = []

for num_layers, nhead in configurations:
    for seq_len in sequence_lengths:
        print(f"Training Transformer with {num_layers} layers and {nhead} heads, Sequence Length: {seq_len}")

        # Prepare data
        train_loader, test_loader = prepare_data(seq_len)

        # Model initialization
        model = TransformerModel(vocab_size, 256, nhead, num_layers, seq_len).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        # Training the model
        start_time = time.time()
        train_loss = train_model(model, train_loader, criterion, optimizer, device)
        end_time = time.time()

        # Evaluation
        test_loss, accuracy = evaluate_model(model, test_loader, criterion, device)
        execution_time = end_time - start_time

        # Collect results
        model_size = sum(p.numel() for p in model.parameters())
        results.append({
            'layers': num_layers,
            'heads': nhead,
            'sequence_length': seq_len,
            'train_loss': train_loss,
            'test_loss': test_loss,
            'accuracy': accuracy,
            'execution_time': execution_time,
            'model_size': model_size
        })

        print(f"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Accuracy: {accuracy:.4f}")
        print(f"Execution Time: {execution_time:.4f} seconds | Model Size: {model_size} parameters")

# Print Summary of Results
print("\nSummary of Results:")
for result in results:
    print(f"Layers: {result['layers']} | Heads: {result['heads']} | Sequence Length: {result['sequence_length']}")
    print(f"Train Loss: {result['train_loss']:.4f} | Test Loss: {result['test_loss']:.4f} | Accuracy: {result['accuracy']:.4f}")
    print(f"Execution Time: {result['execution_time']:.4f} seconds | Model Size: {result['model_size']} parameters")
    print("-" * 50)