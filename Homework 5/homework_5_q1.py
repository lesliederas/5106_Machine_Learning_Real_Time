# -*- coding: utf-8 -*-
"""Homework_5_Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cd2BvT8F1MqrOw0LeVm6WZiqT4_5jxFQ
"""

import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader, random_split
import time

# ========= Step 1: Text Input =========
text = """Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting the next character in a sequence of text based on the characters that precede it..."""

# ========= Step 2: Preprocessing =========
chars = sorted(set(text))
char2idx = {c: i for i, c in enumerate(chars)}
idx2char = {i: c for i, c in enumerate(chars)}
vocab_size = len(chars)
encoded_text = [char2idx[c] for c in text]

# ========= Step 3: Dataset =========
class CharDataset(Dataset):
    def __init__(self, encoded_text, seq_length):
        self.seq_length = seq_length
        self.data = torch.tensor([encoded_text[i:i+seq_length] for i in range(len(encoded_text)-seq_length)], dtype=torch.long)
        self.labels = torch.tensor([encoded_text[i+seq_length] for i in range(len(encoded_text)-seq_length)], dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# ========= Step 4: Models =========
class TransformerCharModel(nn.Module):
    def __init__(self, vocab_size, d_model=128, nhead=2, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = nn.Parameter(torch.zeros(1, 100, d_model))
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, nhead), num_layers
        )
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        x = self.embedding(x) + self.pos_encoder[:, :x.size(1), :]
        x = self.transformer(x)
        return self.fc(x[:, -1, :])  # predict only last character

class LSTMCharModel(nn.Module):
    def __init__(self, vocab_size, d_model=128, hidden_size=256, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.lstm = nn.LSTM(d_model, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.lstm(x)
        return self.fc(x[:, -1, :])

# ========= Step 5: Train + Evaluate =========
def evaluate(model, dataloader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for x, y in dataloader:
            output = model(x)
            preds = output.argmax(dim=1)
            correct += (preds == y).sum().item()
            total += y.size(0)
    return correct / total

def train_model(model, train_loader, val_loader, epochs=10, lr=0.001):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    start_time = time.time()
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for x, y in train_loader:
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        val_acc = evaluate(model, val_loader)
        print(f"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f} | Val Acc: {val_acc:.4f}")

    elapsed = time.time() - start_time
    print(f"Training Time: {elapsed:.2f} sec")
    print(f"Model Size: {sum(p.numel() for p in model.parameters()):,} parameters")

# ========= Step 6: Run Experiments =========
for seq_len in [10, 20, 30]:
    print(f"\n======= SEQ LENGTH: {seq_len} =======")
    dataset = CharDataset(encoded_text, seq_len)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_data, val_data = random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=64)

    # --- Transformer ---
    print(f"\n-- Transformer (2 layers, 2 heads) --")
    transformer_model = TransformerCharModel(vocab_size)
    train_model(transformer_model, train_loader, val_loader)

    # --- LSTM ---
    print(f"\n-- LSTM (2 layers) --")
    lstm_model = LSTMCharModel(vocab_size)
    train_model(lstm_model, train_loader, val_loader)

"""With attention

"""

import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader, random_split
import time

# ========= Step 1: Text Input =========
text = """Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting the next character in a sequence of text based on the characters that precede it..."""

# ========= Step 2: Preprocessing =========
chars = sorted(set(text))
char2idx = {c: i for i, c in enumerate(chars)}
idx2char = {i: c for i, c in enumerate(chars)}
vocab_size = len(chars)
encoded_text = [char2idx[c] for c in text]
class Attention(nn.Module):
    def __init__(self, hidden_size):
        super(Attention, self).__init__()
        self.attn = nn.Linear(hidden_size, 1)

    def forward(self, lstm_output):
        attn_weights = torch.softmax(self.attn(lstm_output), dim=1)
        weighted_sum = torch.sum(attn_weights * lstm_output, dim=1)
        return weighted_sum

class LSTMWithAttentionCharModel(nn.Module):
    def __init__(self, vocab_size, d_model=128, hidden_size=256, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.lstm = nn.LSTM(d_model, hidden_size, num_layers, batch_first=True)
        self.attention = Attention(hidden_size)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)
        attn_out = self.attention(lstm_out)  # Apply attention to LSTM output
        return self.fc(attn_out)

# Example usage in the experiment loop:
for seq_len in [10, 20, 30]:
    print(f"\n======= SEQ LENGTH: {seq_len} =======")
    dataset = CharDataset(encoded_text, seq_len)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_data, val_data = random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=64)

    # --- Transformer ---
    print(f"\n-- Transformer (2 layers, 2 heads) --")
    transformer_model = TransformerCharModel(vocab_size)
    train_model(transformer_model, train_loader, val_loader)

    # --- LSTM with Attention ---
    print(f"\n-- LSTM with Attention (2 layers) --")
    lstm_attn_model = LSTMWithAttentionCharModel(vocab_size)
    train_model(lstm_attn_model, train_loader, val_loader)